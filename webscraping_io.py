# -*- coding: utf-8 -*-
"""webscraping.io.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F8cPY9nlVK8u-JUFkM5DpKJjWbyUXyfW
"""

# importing various libraries
!pip install xlsxwriter
!pip install requests

from bs4 import BeautifulSoup
import pandas as pd
import numpy as mp
import requests
import xlsxwriter
from sre_constants import JUMP
from pickle import HIGHEST_PROTOCOL

    # Forming Dataframe to store and represent Data
my_columns = [ 'Batch' , 'Heading' , 'Location' , 'Date' , 'Link' ]
final_dataframe1 = pd.DataFrame(columns = my_columns)

    # Day loop
x = 44197
index = 0
Batch = 0
# limit = 45291
while x <= 45291:
  daily_url = f'https://timesofindia.indiatimes.com/2021/1/1/archivelist/year-2021,month-1,starttime-{x}.cms'
  Batch += 1
  x += 1

    # Retrieving links from the daily_url
  response = requests.get(daily_url)
  html_content = response.text
  soup = BeautifulSoup(html_content, 'html.parser')

  link = soup.find('tr' , class_ = "rightColWrap")
  links = link.find_all('a')

      #  Retrieving data from links
  local_url = ""
  for link in links:
      index += 1
      local_url = link.get('href')
      response_link = requests.get(local_url)
      html_content = response_link.text
      soup_link = BeautifulSoup(html_content, 'html.parser')

      try:
          # Extracting Heading
        headings = []
        for tag in soup_link.find_all(['h1']):
            headings.append(tag.text.strip())

        sorted_headings = sorted(headings)
        for h1 in sorted_headings:
            Heading = h1

            # Extracting Date
        datex = soup_link.find('div' , class_ = 'xf8Pm byline')
        date_time = datex.find("span").text

        Date = ""
        i = 0
        while ( date_time[i] != 'I' ):
          Date = Date + date_time[i]
          i += 1

            # Extraction of Location
        Location_txt = soup_link.find('div' , class_ = '_s30J clearfix')
        child_txt = Location_txt.contents
        Location = ""
        i = 0
        if ":" in child_txt[0]:
          while( child_txt[0][i] != ':' ):
              Location = Location + child_txt[0][i]
              i += 1
        else:
          Location = "General"
            
        # Some Broken URLs
      except:
          Heading = " "
          Location = " "
          date = " "
      Link = local_url

      # Adding all scraped data in a dataframe 
      final_dataframe1.loc[index] = [
      Batch,
      Heading,
      Location,
      Date,
      Link
      ]

      # To check the continuous running of code.
      # It also saves us from unwanted runtime error, we can jumpm to that Batch(day)
      print(str(Batch) + "," + str(index))
      
# Creating an excel file to store the dataframe
datatoexcel = pd.ExcelWriter('TOI_2021-23.xlsx')
final_dataframe1.to_excel(datatoexcel)
datatoexcel.close()
print('DataFrame is written to Excel File successfully.')
